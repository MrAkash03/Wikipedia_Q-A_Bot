# -*- coding: utf-8 -*-
"""WIki_Q&A Bot using RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F01_kDRthcM-tI1Xluuv23Qtx3gYFsnc

# Wikipedia RAG Q&A Bot  
**Retrieval‚ÄëAugmented Generation** using Hugging‚ÄØFace + Wikipedia API  

---  

## Cell 1 ‚Äì Install dependencies
"""

!pip install -q \
    transformers \
    sentence-transformers \
    faiss-cpu \
    wikipedia \
    torch \
    accelerate \
    bitsandbytes \
    langchain \
    langchain-community \
    langchain-huggingface \
    requests==2.32.4

"""## Cell 2 ‚Äì Imports & Global settings"""

import os
import re
import json
import faiss
import numpy as np
import wikipedia
from tqdm.auto import tqdm

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
EMBEDDER_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Tone handling
def choose_tone(is_factual: bool) -> str:
    return "professional" if is_factual else "friendly"

"""## Cell 3 ‚Äì Build the Wikipedia passage index (once per session)"""

# ---------- 1. Download Wikipedia passages ----------
print("Downloading Wikipedia passages (top 10k articles)...")
wikipedia.set_lang("en")

# Grab a list of popular pages
pages = wikipedia.random(pages=10000)   # ~10k articles ‚Üí ~30‚Äë40k passages

# Split each article into overlapping chunks
def chunk_text(text, chunk_size=256, overlap=64):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
        i += chunk_size - overlap
    return chunks

passages = []
titles   = []

print("Chunking articles...")
for title in tqdm(pages, desc="Chunking"):
    try:
        page = wikipedia.page(title)
        chunks = chunk_text(page.content)
        passages.extend(chunks)
        titles.extend([page.title] * len(chunks))
    except Exception:
        continue   # skip disambiguation / missing pages

print(f"Created {len(passages)} passages from {len(set(titles))} articles.")

"""## Cell 4 ‚Äì Encode passages with Sentence‚ÄëTransformer"""

embedder = SentenceTransformer(EMBEDDER_NAME)
print("Encoding passages (this may take a minute)...")
passage_embeddings = embedder.encode(passages, batch_size=64, show_progress_bar=True, convert_to_numpy=True)

# Normalise for cosine similarity
faiss.normalize_L2(passage_embeddings)

# Build FAISS index
dim = passage_embeddings.shape[1]
index = faiss.IndexFlatIP(dim)   # Inner product = cosine after normalisation
index.add(passage_embeddings)
print(f"FAISS index ready with {index.ntotal} vectors.")

"""## Cell 5 ‚Äì Load the generative LLM (4‚Äëbit quantised for Colab)"""

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype="auto"
)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    max_length=2048,
    temperature=0.7,
    top_p=0.95,
    do_sample=True,
    return_full_text=False,
    truncation=True
)

"""## Cell 6 ‚Äì Retrieval + Generation helper"""

MAX_CONTEXT_TOKENS = 1500   # safe headroom for TinyLlama (max 2048)

def truncate_context_to_tokens(passages_with_meta, query, max_tokens):
    """Keep passages until total token count ‚â§ max_tokens."""
    kept = []
    tokens_used = 0

    # Tokenize query once
    query_ids = tokenizer.encode(query, add_special_tokens=False)
    tokens_used += len(query_ids) + 50
    for passage, title, score in passages_with_meta:
        passage_ids = tokenizer.encode(passage, add_special_tokens=False)
        if tokens_used + len(passage_ids) > max_tokens:
            break
        kept.append((passage, title, score))
        tokens_used += len(passage_ids) + 20
    return kept

def build_prompt(query, contexts):
    # Truncate to safe length
    safe_contexts = truncate_context_to_tokens(contexts, query, MAX_CONTEXT_TOKENS)

    # Simple, clean prompt
    context_str = "\n".join([f"{ctx}" for ctx, _, _ in safe_contexts])
    return f"""Use only the following Wikipedia excerpts to answer the question.
If the answer is not present, reply ‚ÄúI don‚Äôt know.‚Äù

Excerpts:
{context_str}

Question: {query}
Answer:"""

def answer_question(query):
    retrieved = retrieve(query, k=10)                # pull a few more, then truncate
    if retrieved[0][2] < 0.35:                       # low relevance ‚Üí chit‚Äëchat
        return None, retrieved

    prompt = build_prompt(query, retrieved)
    output = generator(prompt, max_new_tokens=256)[0]["generated_text"].strip()
    return output, retrieved

"""## Cell 7 ‚Äì Simple chit‚Äëchat fallback (rule‚Äëbased)"""

GREETINGS = ["hi", "hello", "hey", "good morning", "good afternoon", "good evening"]
BYE_WORDS = ["bye", "goodbye", "see you", "exit", "quit"]

def is_greeting(msg):
    return any(g in msg.lower() for g in GREETINGS)

def is_bye(msg):
    return any(b in msg.lower() for b in BYE_WORDS)

def simple_chat(msg):
    msg = msg.lower().strip()
    if is_greeting(msg):
        return "Hello! I'm your Wikipedia Q&A bot. Ask me anything, or just chat. üòä"
    if "how are you" in msg:
        return "I'm doing great, thanks! Ready to fetch Wikipedia knowledge for you."
    if "your name" in msg:
        return "I'm WikiRAG, a Retrieval‚ÄëAugmented Generation bot built with Hugging Face."
    if "joke" in msg:
        return "Why did the computer go to art school? Because it wanted to learn how to draw a better *byte*! üòÑ"
    return "That's interesting! Anything specific you'd like to know from Wikipedia?"

"""## Cell 8 ‚Äì Main chat loop (run this cell to start!)"""

print("\n" + "="*50)
print("   ü§ñ  Wikipedia RAG Q&A Bot  ü§ñ")
print("="*50)
print("Type your question or just chat. Say **bye** to exit.\n")

while True:
    user_input = input("You: ").strip()
    if not user_input:
        continue

    if is_bye(user_input):
        tone = choose_tone(False)
        print("Bot: Goodbye! Feel free to come back anytime. üëã\n")
        break

    # 1. Try factual answer
    answer, retrieved = answer_question(user_input)
    if answer:
        tone = choose_tone(True)
        print(f"Bot ({tone}): {answer}\n")
        # optional: show sources
        sources = set(title for _, title, _ in retrieved)
        print(f"Sources: {', '.join(sorted(sources))}\n")
    else:
        # 2. Fallback to chit‚Äëchat
        chat_reply = simple_chat(user_input)
        tone = choose_tone(False)
        print(f"Bot ({tone}): {chat_reply}\n")

"""## Cell 8 ‚Äì Save the Model"""

# Example save
import pickle, json
np.save("embeddings.npy", passage_embeddings)
with open("passages.pkl", "wb") as f: pickle.dump(passages, f)
with open("titles.pkl", "wb") as f: pickle.dump(titles, f)